{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting CUDA_VISIBLE_DEVICES =  0,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import scipy.misc\n",
    "\n",
    "import dense_correspondence_manipulation.utils.utils as utils\n",
    "utils.add_dense_correspondence_to_python_path()\n",
    "\n",
    "import dense_correspondence\n",
    "from dense_correspondence.evaluation.evaluation import *\n",
    "import dense_correspondence.correspondence_tools.correspondence_plotter as correspondence_plotter\n",
    "import dense_correspondence.correspondence_tools.correspondence_finder as correspondence_finder\n",
    "from dense_correspondence.correspondence_tools.correspondence_finder import random_sample_from_masked_image, pinhole_projection_image_to_world\n",
    "from dense_correspondence.dataset.dense_correspondence_dataset_masked import ImageType\n",
    "from dense_correspondence.network.dense_correspondence_network import DenseCorrespondenceNetwork\n",
    "import dense_correspondence.evaluation.plotting as dc_plotting\n",
    "%matplotlib inline \n",
    "utils.set_cuda_visible_devices([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidtseng/pytorch-dense-correspondence/env/local/lib/python2.7/site-packages/torch/nn/modules/module.py:482: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  own_state[name].copy_(param)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SpartanDataset:\n",
      "   - in train mode\n",
      "   - number of scenes 126\n",
      "   - total images:     78246\n"
     ]
    }
   ],
   "source": [
    "config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', \n",
    "                               'dense_correspondence', 'evaluation', 'evaluation.yaml')\n",
    "config = utils.getDictFromYamlFilename(config_filename)\n",
    "dce = DenseCorrespondenceEvaluation(config)\n",
    "DCE = DenseCorrespondenceEvaluation\n",
    "# dcn = dce.load_network_from_config(\"cup_bottlesauce_elephant_cat_pear_unit_16\") # this worked pretty well\n",
    "dcn = dce.load_network_from_config(\"thingiverse_multi_rgb_unit_16\") \n",
    "\n",
    "dataset = dcn.load_training_dataset()\n",
    "camera_intrinsics = dataset.get_camera_intrinsics(\"elephant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_index_and_norm_diff(query, features, cond=None):\n",
    "    \"\"\"\n",
    "    Assumes that query and features both have shape (N, D).\n",
    "    query: ndarray (D,)\n",
    "    features: ndarray (N, D)    \n",
    "    \"\"\"\n",
    "    \n",
    "    if cond is None: \n",
    "        norm_diffs = np.sqrt(np.sum(np.square(features - query), axis=1)) \n",
    "        \n",
    "    elif cond == \"ppf\":\n",
    "        f1 = features[:, 1:]\n",
    "        D = f1.shape[1]\n",
    "        f2 = np.concatenate((f1[:, D//2:], f1[:, :D//2]), axis=1)\n",
    "                \n",
    "        norm_diffs1 = np.sqrt(np.sum(np.square(f1 - query[1:]), axis=1))\n",
    "        norm_diffs2 = np.sqrt(np.sum(np.square(f2 - query[1:]), axis=1))\n",
    "        norm_diffs = np.minimum(norm_diffs1, norm_diffs2)\n",
    "    \n",
    "        feature_cond = features[:, 0]\n",
    "        query_cond = query[0]\n",
    "        mask = np.logical_and(feature_cond > 5./7*query_cond, feature_cond < 9./7*query_cond).astype(np.float)\n",
    "        norm_diffs = mask * norm_diffs + np.nan_to_num((1-mask) * float(\"inf\") * np.ones(norm_diffs.shape))\n",
    "    else:\n",
    "        assert 1 == 0 \n",
    "        \n",
    "    best_match_index = np.argmin(norm_diffs)\n",
    "    best_match_diff = norm_diffs[best_match_index]\n",
    "    result = np.array([best_match_index, best_match_diff])\n",
    "        \n",
    "    #         candidate_indices = np.argwhere(np.logical_and(ppfs[:,0] > 5/7*ppf_target[0], ppfs[:,0] < 9/7*ppf_target[0])).T[0]\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "def simple_match(f1, f2, cond=None):\n",
    "    \"\"\"\n",
    "    Simple brute force matching between features1 and features2.\n",
    "    f1: ndarray of shape (N, ... )\n",
    "    f2: ndarray of shape (N, ....)\n",
    "    fn: None, \"ppf\"\n",
    "    Returns indices of matches. Also does some filtering \n",
    "    to make sure that the best match of f1 is the best match of f2. \n",
    "    \"\"\"\n",
    "    \n",
    "    f1 = f1.reshape((f1.shape[0], -1)) \n",
    "    f2 = f2.reshape((f2.shape[0], -1))\n",
    "\n",
    "    f1_matches = np.apply_along_axis(lambda x: match_index_and_norm_diff(x, f2, cond), 1, f1)\n",
    "    f2_matches = np.apply_along_axis(lambda x: match_index_and_norm_diff(x, f1, cond), 1, f2)\n",
    "    \n",
    "    best_matches = []\n",
    "    for i in range(f1_matches.shape[0]):\n",
    "        norm_diff = f1_matches[i, 1]\n",
    "        best_match_index = int(f1_matches[i, 0])\n",
    "        if f2_matches[best_match_index, 0] == i: # If they're the best matches of each other\n",
    "            best_matches.append((i, best_match_index, norm_diff))\n",
    "            \n",
    "    return best_matches\n",
    "\n",
    "\n",
    "def draw_orb_matches(img1, kp1, des1, img2, kp2, des2, match_type=\"hamming\", num_matches=10):\n",
    "    dist = 0.01 # placeholder constant for opencv\n",
    "\n",
    "    if match_type == \"hamming\": \n",
    "        print(\"Using openCV norm_hammings\")\n",
    "        # create BFMatcher object\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        # Match descriptors.    \n",
    "        matches = bf.match(des1,des2)\n",
    "    else:\n",
    "        print(\"Using simple match\")\n",
    "        best_matches_lst = simple_match(des1, des2)\n",
    "        matches = [cv2.DMatch(x[0], x[1], x[2]) for x in best_matches_lst]\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    # Draw first 10 matches.\n",
    "    img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:num_matches], None, flags=2)\n",
    "\n",
    "    plt.figure(figsize = (20, 10))\n",
    "    plt.imshow(img3)\n",
    "    plt.show()\n",
    "    \n",
    "def run_dcn_on_image(dcn, dataset, img):\n",
    "    \"\"\"\n",
    "    Run DCN on PIL img. use of dataset is a hack, just need to ge the descriptor image stats \n",
    "    which actually just uses a constant for std and mean. \n",
    "    \"\"\"\n",
    "    rgb_a_tensor = dataset.rgb_image_to_tensor(img)\n",
    "    res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n",
    "    descriptor_image_stats = dcn.descriptor_image_stats\n",
    "    res_a_norm = dc_plotting.normalize_descriptor(res_a, descriptor_image_stats[\"mask_image\"])\n",
    "    return res_a_norm\n",
    "\n",
    "def run_ORB(heap, target, num_matches=5):\n",
    "    \"\"\"Run baseline ORB. \"\"\"\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    kp1, des1 = orb.detectAndCompute(target,None)\n",
    "    kp2, des2 = orb.detectAndCompute(heap,None)\n",
    "    \n",
    "    draw_orb_matches(target, kp1, des1, heap, kp2, des2, num_matches=num_matches)\n",
    "\n",
    "def run_ORB_and_dense_descriptors(dcn, dataset, heap_name, target_name, num_matches=5, image_type=\"depth\"):\n",
    "    \"\"\"Use ORB keypoints + dense descriptors. \"\"\"\n",
    "    orb = cv2.ORB_create()\n",
    "    \n",
    "    if image_type == \"rgb\":\n",
    "        target_img = Image.open(\"images/targets/{}/target_rgb.png\".format(target_name)).convert('RGB')\n",
    "    else:\n",
    "        target_img = Image.open(\"images/targets/{}/target_depth.png\".format(target_name)).convert('RGB')\n",
    "    target = np.array(target_img)\n",
    "    target_descriptors = run_dcn_on_image(dcn, dataset, target_img)\n",
    "    target_mask = np.load(\"images/targets/{}/target_mask.npy\".format(target_name)).astype(np.bool)\n",
    "\n",
    "    if image_type == \"rgb\":\n",
    "        heap_img_rgb = Image.open(\"images/heaps/{}/heap_rgb.png\".format(heap_name)).convert('RGB')\n",
    "    else:\n",
    "        heap_img = Image.open(\"images/heaps/{}/heap_depth.png\".format(heap_name)).convert('RGB')\n",
    "    heap = np.array(heap_img)\n",
    "    heap_descriptors = run_dcn_on_image(dcn, dataset, heap_img)\n",
    "    obj1_mask = np.load(\"images/heaps/{}/obj1_mask.npy\".format(heap_name)).astype(np.bool)\n",
    "    obj2_mask = np.load(\"images/heaps/{}/obj2_mask.npy\".format(heap_name)).astype(np.bool)\n",
    "    \n",
    "    # be careful, OpenCV format is  (u,v) = (right, down)\n",
    "    kp1_map = lambda x: target_descriptors[x[1], x[0]]\n",
    "    kp2_map = lambda x: heap_descriptors[x[1], x[0]]\n",
    "\n",
    "    # # find the keypoints with ORB\n",
    "    kp1 = orb.detect(target,None)\n",
    "    kp1_array = np.array([x.pt for x in kp1]).reshape(-1, 2).astype(np.int)\n",
    "    des1 = np.apply_along_axis(kp1_map, 1, kp1_array)\n",
    "    \n",
    "    kp2 = orb.detect(heap,None)\n",
    "    kp2_array = np.array([x.pt for x in kp2]).reshape(-1, 2).astype(np.int)\n",
    "    des2 = np.apply_along_axis(kp2_map, 1, kp2_array)\n",
    "    draw_orb_matches(target, kp1, des1, heap, kp2, des2, num_matches=num_matches, match_type=\"simple\")\n",
    "\n",
    "def run_dense_descriptor_original_match(dcn, dataset, heap_name, target_name, num_matches=5, patch_size=None, image_type=\"depth\"):\n",
    "    \"\"\"Use the original dense descriptor sampling + matching method\"\"\"\n",
    "    \n",
    "    if image_type == \"rgb\":\n",
    "        target_img = Image.open(\"images/targets/{}/target_rgb.png\".format(target_name)).convert('RGB')\n",
    "    else:\n",
    "        target_img = Image.open(\"images/targets/{}/target_depth.png\".format(target_name)).convert('RGB')\n",
    "    target = np.array(target_img)\n",
    "    target_descriptors = run_dcn_on_image(dcn, dataset, target_img)\n",
    "    target_mask = np.load(\"images/targets/{}/target_mask.npy\".format(target_name)).astype(np.bool)\n",
    "    \n",
    "    if image_type == \"rgb\":\n",
    "        heap_img_rgb = Image.open(\"images/heaps/{}/heap_rgb.png\".format(heap_name)).convert('RGB')\n",
    "    else:\n",
    "        heap_img = Image.open(\"images/heaps/{}/heap_depth.png\".format(heap_name)).convert('RGB')    \n",
    "    heap = np.array(heap_img)\n",
    "    heap_descriptors = run_dcn_on_image(dcn, dataset, heap_img)\n",
    "    heap_mask = np.load(\"images/heaps/{}/heap_mask.npy\".format(heap_name)).astype(np.bool)\n",
    "    \n",
    "    DCE.single_image_pair_qualitative_analysis(dcn, dataset, target, heap, target_mask, heap_mask, num_matches, patch_size)\n",
    "\n",
    "def run_dense_descriptor_original_match_with_filtering(dcn, dataset, target, heap, target_mask, heap_mask, target_descriptors, heap_descriptors, top_num_matches=5):\n",
    "    \"\"\"Use the original dense descriptor sampling + matching method + some filtering\"\"\"    \n",
    "    num_heap_samples = 1000\n",
    "    num_target_samples = 400\n",
    "    \n",
    "    heap_sampled_idx_list = random_sample_from_masked_image(heap_mask[:, :, 0].astype(np.int), num_heap_samples)\n",
    "    \n",
    "    target_sampled_idx_list = random_sample_from_masked_image(target_mask[:, :, 0].astype(np.int), num_target_samples)\n",
    "    \n",
    "    heap_sample_descriptors = []\n",
    "    for i in range(num_heap_samples):\n",
    "        heap_sample_descriptors.append(heap_descriptors[heap_sampled_idx_list[0][i], heap_sampled_idx_list[1][i], :])\n",
    "    target_sample_descriptors = []\n",
    "    for i in range(num_target_samples):\n",
    "        target_sample_descriptors.append(target_descriptors[target_sampled_idx_list[0][i], target_sampled_idx_list[1][i], :])\n",
    "    heap_sample_descriptors = np.array(heap_sample_descriptors)\n",
    "    target_sample_descriptors = np.array(target_sample_descriptors)\n",
    "    \n",
    "    best_matches_lst = simple_match(target_sample_descriptors, heap_sample_descriptors)\n",
    "    best_matches_lst = sorted(best_matches_lst, key=lambda x: x[2])\n",
    "    \n",
    "    kp1, kp2, matches = [], [], []\n",
    "    hist_x = []\n",
    "    hist_y = []\n",
    "    hist_values = []\n",
    "    \n",
    "    diam = 0.01 \n",
    "    for x, match in enumerate(best_matches_lst):\n",
    "        i, j, dist = match    \n",
    "        kp1.append(cv2.KeyPoint(target_sampled_idx_list[1][i], target_sampled_idx_list[0][i], diam))\n",
    "        kp2.append(cv2.KeyPoint(heap_sampled_idx_list[1][j], heap_sampled_idx_list[0][j], diam))\n",
    "        \n",
    "        hist_x.append(heap_sampled_idx_list[1][j])\n",
    "        hist_y.append(heap_sampled_idx_list[0][j])\n",
    "        hist_values.append(dist)\n",
    "        \n",
    "        matches.append(cv2.DMatch(x, x, dist))\n",
    "\n",
    "#     matches = sorted(matches, key = lambda x:x.distance)\n",
    "    \n",
    "    img3 = cv2.drawMatches(target,kp1,heap,kp2,matches[:top_num_matches], None, flags=2)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    plt.imshow(img3)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(hist_x[:top_num_matches], hist_y[:top_num_matches], color='r', alpha=0.5)\n",
    "    plt.scatter(hist_x[:top_num_matches], hist_y[:top_num_matches], c=hist_values[:top_num_matches],  cmap=\"bwr_r\", alpha=0.75)\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.imshow(heap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for heap_num in range(50):\n",
    "    # run_dense_descriptor_original_match(dcn, dataset, )\n",
    "#     heap_num = 0\n",
    "    print(\"Heap: {}\".format(heap_num))\n",
    "    dirname = \"/home/davidtseng/pytorch-dense-correspondence/dense_correspondence/experiments/mech_search/images/heatmap/10_objs_1_0_ratio_perturbed_80deg\"\n",
    "    # 80 deg\n",
    "    target_img = Image.open(dirname + \"/{}/target/rgb.png\".format(heap_num)).convert('RGB')\n",
    "    target = np.array(target_img)\n",
    "    target_mask = np.load(dirname + \"/{}/target/mask.npy\".format(heap_num)).astype(np.bool)\n",
    "    target_descriptors = run_dcn_on_image(dcn, dataset, target_img)\n",
    "\n",
    "    heap_img = Image.open(dirname + \"/{}/heap/rgb.png\".format(heap_num)).convert('RGB')\n",
    "    heap = np.array(heap_img)\n",
    "    heap_mask = np.load(dirname + \"/{}/heap/mask.npy\".format(heap_num)).astype(np.bool)\n",
    "    heap_descriptors = run_dcn_on_image(dcn, dataset, heap_img)\n",
    "\n",
    "\n",
    "    # DCE.single_image_pair_qualitative_analysis(dcn, dataset, target, heap, target_mask, heap_mask, num_matches=20, patch_size=None)\n",
    "    run_dense_descriptor_original_match_with_filtering(dcn, dataset, target, heap, target_mask, heap_mask, target_descriptors, heap_descriptors, top_num_matches=20)\n",
    "    # run_ORB(heap, target, num_matches=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
